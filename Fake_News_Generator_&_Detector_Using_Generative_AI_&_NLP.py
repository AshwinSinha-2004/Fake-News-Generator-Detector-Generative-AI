# -*- coding: utf-8 -*-
"""Fake_News_Generator_&_Detector_using_Generative-AI_&_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R0_HjFmBqhjkqyAKCKLSWij0I2TLrzh6
"""

!pip install transformers torch scikit-learn nltk pandas numpy

import torch
from transformers import (
    GPT2LMHeadModel, GPT2Tokenizer,
    BertForSequenceClassification, BertTokenizer
)
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import re
import nltk
from nltk.corpus import stopwords

# Download NLTK resources
nltk.download('stopwords')

# Load datasets
real_df = pd.read_csv('True_News.csv')
fake_df = pd.read_csv('Fake_News.csv')

# Extract and preprocess headlines
def preprocess_text(text):
    text = re.sub(r'[^a-zA-Z0-9\s]', '', str(text))
    text = text.lower()
    return text

real_headlines = real_df['title'].apply(preprocess_text).tolist()
fake_headlines = fake_df['title'].apply(preprocess_text).tolist()

# Split data
real_train, real_test = train_test_split(real_headlines, test_size=0.2, random_state=42)
fake_train, fake_test = train_test_split(fake_headlines, test_size=0.2, random_state=42)

class HeadlineGenerator:
    def __init__(self, model_name="gpt2"):
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

    def train(self, headlines, epochs=2, batch_size=8, lr=5e-5):
        self.model.train()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)

        encodings = self.tokenizer(
            headlines,
            truncation=True,
            padding=True,
            max_length=64,
            return_tensors="pt"
        )

        dataset = torch.utils.data.TensorDataset(
            encodings['input_ids'],
            encodings['attention_mask'],
            encodings['input_ids']
        )
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            epoch_loss = 0
            for inputs, mask, labels in loader:
                inputs = inputs.to(self.model.device)
                mask = mask.to(self.model.device)
                labels = labels.to(self.model.device)

                optimizer.zero_grad()
                outputs = self.model(inputs, attention_mask=mask, labels=labels)
                loss = outputs.loss
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            print(f"Epoch {epoch+1}/{epochs} Loss: {epoch_loss/len(loader):.4f}")

    def generate(self, prompt="", max_length=25, num_samples=5, temperature=0.9):
        self.model.eval()
        generated = []

        with torch.no_grad():
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                num_return_sequences=num_samples,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.2,
                pad_token_id=self.tokenizer.eos_token_id,
                do_sample=True
            )
            for output in outputs:
                text = self.tokenizer.decode(output, skip_special_tokens=True)
                text = re.sub(r'\s+', ' ', text).strip()
                text = text.split('.')[0]
                generated.append(text)
        return generated

# Train generator
print("Training Fake News Generator...")
generator = HeadlineGenerator()
generator.train(real_train, epochs=2, batch_size=4)

# Generate headlines
generated_headlines = generator.generate(
    prompt="Breaking news:",
    max_length=25,
    num_samples=5,
    temperature=0.95
)

class NewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=64):
        self.encodings = tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors="pt"
        )
        self.labels = torch.tensor(labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.encodings['input_ids'][idx],
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': self.labels[idx]
        }

    def __len__(self):
        return len(self.labels)

class FakeNewsDetector:
    def __init__(self, model_name="bert-base-uncased"):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2
        ).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

    def prepare_data(self, real_headlines, fake_headlines):
        texts = real_headlines + fake_headlines
        labels = [0] * len(real_headlines) + [1] * len(fake_headlines)

        X_train, X_val, y_train, y_val = train_test_split(
            texts, labels, test_size=0.2, random_state=42
        )

        train_dataset = NewsDataset(X_train, y_train, self.tokenizer)
        val_dataset = NewsDataset(X_val, y_val, self.tokenizer)

        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=16)

        return train_loader, val_loader

    def train(self, train_loader, val_loader, epochs=2, lr=2e-5):
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.model.train()

        for epoch in range(epochs):
            epoch_loss = 0
            for batch in train_loader:
                inputs = {k: v.to(self.model.device) for k, v in batch.items()}
                outputs = self.model(**inputs)
                loss = outputs.loss
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                epoch_loss += loss.item()

            val_acc = self.evaluate(val_loader)
            print(f"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f}")

    def evaluate(self, data_loader):
        self.model.eval()
        predictions, true_labels = [], []

        with torch.no_grad():
            for batch in data_loader:
                inputs = {k: v.to(self.model.device) for k, v in batch.items()}
                labels = inputs.pop('labels')
                outputs = self.model(**inputs)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=1)
                predictions.extend(preds.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())

        return accuracy_score(true_labels, predictions)

    def predict(self, headline):
        self.model.eval()
        encoding = self.tokenizer(
            headline,
            truncation=True,
            padding=True,
            max_length=64,
            return_tensors="pt"
        ).to(self.model.device)

        with torch.no_grad():
            outputs = self.model(**encoding)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=1).item()

        return pred

# Train detector
print("\nTraining Fake News Detector...")
detector = FakeNewsDetector()
train_loader, val_loader = detector.prepare_data(real_train, fake_train)
detector.train(train_loader, val_loader, epochs=2)

# Gradio GUI Implementation
import gradio as gr

# Preprocessing function for input text
def preprocess_input(text):
    text = re.sub(r'[^a-zA-Z0-9\s]', '', str(text))
    text = text.lower()
    return text

# Function to generate headlines
def generate_headlines(num_headlines, prompt=""):
    processed_prompt = preprocess_input(prompt)
    num_headlines = int(num_headlines)
    headlines = generator.generate(
        prompt=processed_prompt,
        max_length=25,
        num_samples=num_headlines,
        temperature=0.95
    )
    return "\n".join([f"{i+1}. {h}" for i, h in enumerate(headlines)])

# Function to detect fake news
def detect_fake(headline):
    processed_headline = preprocess_input(headline)
    prediction = detector.predict(processed_headline)
    return "FAKE" if prediction == 1 else "REAL"

# Create the Gradio interface
with gr.Blocks(title="Fake News Generator & Detector") as demo:
    gr.Markdown("# üóûÔ∏è Fake News Generator & Detector")

    with gr.Tab("Generate Fake News"):
        gr.Markdown("## Generate Fake News Headlines")
        num_headlines = gr.Slider(1, 10, value=5, step=1, label="Number of Headlines to Generate")
        prompt = gr.Textbox(label="Starting Prompt (Optional)", value="Breaking news:")
        generate_btn = gr.Button("Generate Headlines")
        output_headlines = gr.Textbox(label="Generated Headlines", lines=6, interactive=False)

        generate_btn.click(
            fn=generate_headlines,
            inputs=[num_headlines, prompt],
            outputs=output_headlines
        )

    with gr.Tab("Detect Fake News"):
        gr.Markdown("## Detect Fake News Headlines")
        headline_input = gr.Textbox(label="Enter Headline to Analyze")
        detect_btn = gr.Button("Check Authenticity")
        result = gr.Label(label="Detection Result")

        detect_btn.click(
            fn=detect_fake,
            inputs=headline_input,
            outputs=result
        )

    gr.Markdown("### System Information")
    gr.Markdown(f"Running on {'GPU üî•' if torch.cuda.is_available() else 'CPU üê¢'} | "
                f"Generator: GPT-2 | Detector: BERT")

# Launch the interface
print("\n=== Launching Fake News Generation & Detection System ===")
demo.launch()